{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Bidirectional\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.utils import class_weight\n\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\nDATA_PATH = \"/kaggle/input/ravdess-emotional-speech-audio\"\n\ndef extract_features(file_path, mfcc=True, chroma=True, mel=True):\n    y, sr = librosa.load(file_path, sr=None)\n    features = np.array([])\n    \n    if mfcc:\n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n        mfccs = np.mean(mfccs.T, axis=0)\n        features = np.hstack((features, mfccs))\n    \n    if chroma:\n        stft = np.abs(librosa.stft(y))\n        chroma = librosa.feature.chroma_stft(S=stft, sr=sr)\n        chroma = np.mean(chroma.T, axis=0)\n        features = np.hstack((features, chroma))\n    \n    if mel:\n        mel = librosa.feature.melspectrogram(y=y, sr=sr)\n        mel = np.mean(mel.T, axis=0)\n        features = np.hstack((features, mel))\n    \n    return features\n\nemotions = {\n    '01': 'neutral',\n    '02': 'calm',\n    '03': 'happy',\n    '04': 'sad',\n    '05': 'angry',\n    '06': 'fearful',\n    '07': 'disgust',\n    '08': 'surprised'\n}\n\nX, y = [], []\n\nfor actor in os.listdir(DATA_PATH):\n    actor_path = os.path.join(DATA_PATH, actor)\n    for file in os.listdir(actor_path):\n        file_path = os.path.join(actor_path, file)\n        if os.path.isfile(file_path) and file_path.endswith('.wav'):\n            features = extract_features(file_path)\n            X.append(features)\n            emotion_code = file.split(\"-\")[2]\n            y.append(emotions[emotion_code])\n\nX = np.array(X)\ny = np.array(y)\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)\ny = to_categorical(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclass_weights = class_weight.compute_class_weight(class_weight='balanced',\n                                                  classes=np.unique(y_train.argmax(axis=1)),\n                                                  y=y_train.argmax(axis=1))\nclass_weights = dict(enumerate(class_weights))\n\nX_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\n\nmodel = Sequential()\n\nmodel.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X_train.shape[1], 1), kernel_regularizer=l2(0.001)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.001)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(0.4))\n\nmodel.add(Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(0.001))))\nmodel.add(Bidirectional(GRU(128, kernel_regularizer=l2(0.001))))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(y_train.shape[1], activation='softmax'))\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), \n                    callbacks=[early_stopping, reduce_lr], class_weight=class_weights, verbose=1)\n\ny_pred = model.predict(X_test)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_true_labels = np.argmax(y_test, axis=1)\n\naccuracy = accuracy_score(y_true_labels, y_pred_labels)\nf1 = f1_score(y_true_labels, y_pred_labels, average='macro')\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1 Score: {f1}\")\n\nconf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}